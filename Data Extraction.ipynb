{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook will scrap the data from subreddit r/India and store it in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import praw #PRAW is the API being used to scrap data from Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a reddit instance by authenticating ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='xxxx', client_secret='xxxx', user_agent='Reddit WebScrapping', password='xxxx', username='shreyagupta0806')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The client_id, client_secret and password are anonymised to prevent privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we will extract information from the subreddit r/India"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get subreddit info of india subreddit\n",
    "india_subreddit = reddit.subreddit('India')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will extract useful columns from the posts in this subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: we extract the \"hottest\" posts, since these are the posts that received the maximum upvotes in the closest time from when they were put up.\n",
    "\n",
    "Currently, we will extract 1000 posts for reference. This number can be increased by changing the numPosts variable intialised below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fqqdsg</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fqqdsg...</td>\n",
       "      <td>Coronavirus (COVID-19) Megathread - News and U...</td>\n",
       "      <td>**Central thread for sharing coronavirus News ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fo0xj9</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fo0xj9...</td>\n",
       "      <td>Coronavirus (Covid-19) Multi-Lingual Resources...</td>\n",
       "      <td>This post will serve as a wiki for multi-lingu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fxm0xq</td>\n",
       "      <td>https://v.redd.it/o8y1vddg2qr41</td>\n",
       "      <td>Woke up to this today. Made my day!</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fxpkhy</td>\n",
       "      <td>https://www.thequint.com/neon/covid-19-bhopal-...</td>\n",
       "      <td>Bhopal Doctor Lives In Car To Protect Family F...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fxqifi</td>\n",
       "      <td>https://theprint.in/india/rss-says-tablighi-ja...</td>\n",
       "      <td>RSS says Tablighi Jamaat conduct not reflectio...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                                url  \\\n",
       "0  fqqdsg  https://www.reddit.com/r/india/comments/fqqdsg...   \n",
       "1  fo0xj9  https://www.reddit.com/r/india/comments/fo0xj9...   \n",
       "2  fxm0xq                    https://v.redd.it/o8y1vddg2qr41   \n",
       "3  fxpkhy  https://www.thequint.com/neon/covid-19-bhopal-...   \n",
       "4  fxqifi  https://theprint.in/india/rss-says-tablighi-ja...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Coronavirus (COVID-19) Megathread - News and U...   \n",
       "1  Coronavirus (Covid-19) Multi-Lingual Resources...   \n",
       "2                Woke up to this today. Made my day!   \n",
       "3  Bhopal Doctor Lives In Car To Protect Family F...   \n",
       "4  RSS says Tablighi Jamaat conduct not reflectio...   \n",
       "\n",
       "                                             content  \n",
       "0  **Central thread for sharing coronavirus News ...  \n",
       "1  This post will serve as a wiki for multi-lingu...  \n",
       "2                                                     \n",
       "3                                                     \n",
       "4                                                     "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. extract x hottest posts' headline, id, url, content\n",
    "def get_hottest_posts(numPosts):\n",
    "    \"\"\"\n",
    "    Takes the number of posts we want to scrap and returns the\n",
    "    pandas dataframe consisting of the post id (unique to a \n",
    "    post - will be used in the next step), post url (can be\n",
    "    important in the coming steps), post title (or the headline)\n",
    "    and its content (i.e. what the post contained)\n",
    "    \"\"\"\n",
    "    for post in india_subreddit.hot(limit=numPosts):\n",
    "        posts.append([post.id, post.url, post.title, post.selftext])\n",
    "\n",
    "    return pd.DataFrame(posts, columns=['id', 'url', 'title', 'content'])\n",
    "\n",
    "posts = []\n",
    "numPosts = 2000\n",
    "posts = get_hottest_posts(numPosts)\n",
    "\n",
    "#displaying the first five rows of the posts dataframe\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have the basic attributes of a post, we will extract the flair of the post. \n",
    "\n",
    "Note: Flair is the category that the post belonged to. This category (for the India subreddit) is assigned by the moderator or community members of the subreddit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coronavirus', 'Coronavirus', 'Non-Political', 'Coronavirus', 'Coronavirus']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. extract the flairs of these posts\n",
    "def get_post_flairs(posts):\n",
    "    \"\"\"\n",
    "    This function takes the posts extracted in get_hottest_posts\n",
    "    function and returns the flairs of these posts using the post\n",
    "    id.\n",
    "    \"\"\"\n",
    "    flairs = []\n",
    "    for i in range(posts.shape[0]):\n",
    "        submission = reddit.submission(id=posts.id[i])\n",
    "        flairs.append(submission.link_flair_text)\n",
    "    return flairs\n",
    "    \n",
    "flairs = get_post_flairs(posts)\n",
    "\n",
    "#displaying the flairs of the first five posts\n",
    "flairs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we append these flairs to our post data obtained from the get_hottest_posts function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fqqdsg</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fqqdsg...</td>\n",
       "      <td>Coronavirus (COVID-19) Megathread - News and U...</td>\n",
       "      <td>**Central thread for sharing coronavirus News ...</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fo0xj9</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fo0xj9...</td>\n",
       "      <td>Coronavirus (Covid-19) Multi-Lingual Resources...</td>\n",
       "      <td>This post will serve as a wiki for multi-lingu...</td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fxm0xq</td>\n",
       "      <td>https://v.redd.it/o8y1vddg2qr41</td>\n",
       "      <td>Woke up to this today. Made my day!</td>\n",
       "      <td></td>\n",
       "      <td>Non-Political</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fxpkhy</td>\n",
       "      <td>https://www.thequint.com/neon/covid-19-bhopal-...</td>\n",
       "      <td>Bhopal Doctor Lives In Car To Protect Family F...</td>\n",
       "      <td></td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fxqifi</td>\n",
       "      <td>https://theprint.in/india/rss-says-tablighi-ja...</td>\n",
       "      <td>RSS says Tablighi Jamaat conduct not reflectio...</td>\n",
       "      <td></td>\n",
       "      <td>Coronavirus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                                url  \\\n",
       "0  fqqdsg  https://www.reddit.com/r/india/comments/fqqdsg...   \n",
       "1  fo0xj9  https://www.reddit.com/r/india/comments/fo0xj9...   \n",
       "2  fxm0xq                    https://v.redd.it/o8y1vddg2qr41   \n",
       "3  fxpkhy  https://www.thequint.com/neon/covid-19-bhopal-...   \n",
       "4  fxqifi  https://theprint.in/india/rss-says-tablighi-ja...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Coronavirus (COVID-19) Megathread - News and U...   \n",
       "1  Coronavirus (Covid-19) Multi-Lingual Resources...   \n",
       "2                Woke up to this today. Made my day!   \n",
       "3  Bhopal Doctor Lives In Car To Protect Family F...   \n",
       "4  RSS says Tablighi Jamaat conduct not reflectio...   \n",
       "\n",
       "                                             content          flair  \n",
       "0  **Central thread for sharing coronavirus News ...    Coronavirus  \n",
       "1  This post will serve as a wiki for multi-lingu...    Coronavirus  \n",
       "2                                                     Non-Political  \n",
       "3                                                       Coronavirus  \n",
       "4                                                       Coronavirus  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. append posts with their flairs to get the resultant dataset\n",
    "\n",
    "posts['flair'] = flairs\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(758, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how many posts were extracted\n",
    "posts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We might also want to see what were the potential flairs that could have been assigned in this subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible flairs: \n",
      "\n",
      "1. Politics\n",
      "2. Non-Political\n",
      "3. AskIndia\n",
      "4. Policy/Economy\n",
      "5. Business/Finance\n",
      "6. Science/Technology\n",
      "7. Scheduled\n",
      "8. Sports\n",
      "9. Food\n",
      "10. Photography\n",
      "11. CAA-NRC-NPR\n",
      "12. Coronavirus\n"
     ]
    }
   ],
   "source": [
    "def print_possible_flairs(subreddit):\n",
    "    \n",
    "    print(\"Possible flairs: \\n\")\n",
    "    i = 0\n",
    "\n",
    "    for template in subreddit.flair.link_templates:\n",
    "        print(str(i+1) + \". \" + template[\"text\"])\n",
    "        i += 1\n",
    "    \n",
    "print_possible_flairs(india_subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the possible flairs that can be assigned to a post by a moderator or community member of r/India"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we finally extracted the data from subreddit of India using praw. The only doubt I have here is why did 752 posts get extracted even though I extracted 1000 posts. Possible guesses: \n",
    "1. Either there were only 752 posts in the \"hottest\" category, or\n",
    "2. There is some stopping condition on the number of posts that can be extracted\n",
    "752 seems like a weird number for the second scenario, and I'm not sure about the first. So will probably get back to it, after completing the overall task, time remaining. Till then I think 752 should be a good number, if we need more, we'll get it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, saving our extracted data in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5: save the data in a csv\n",
    "posts.to_csv('dataset.csv', index=False) \n",
    "\n",
    "#index=False will prevent the row numbers from being saved as an independent column/attribute in the saved csv file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
