{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook will contain a sequential list of how I approached the problem and everything that worked/failed; things I struggled with and how I found solutions to them.\n",
    "\n",
    "might also feature some rant and superflous commentary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To be submitted:\n",
    "Links to\n",
    " - public github repository\n",
    " - Heroku application\n",
    " - automated check endpoint.\n",
    " \n",
    "## Deadline: April 26, 2020 \n",
    "\n",
    "## Tasks:\n",
    "1. Download Reddit data from https://www.reddit.com/r/india/\n",
    "2. Data analysis\n",
    "3. Build Flare Detector\n",
    "4. Build Web Application\n",
    "5. Host on Heroku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 1: April 7, 2020 (T-19 days)\n",
    "\n",
    "- read up on various licenses for the repository - https://choosealicense.com is a good link for the same (for future purposes) - chose MIT License since the project is simple and does not require closed source versions.\n",
    "- Understood the problem statement:\n",
    "    - Since the task is to build a flair (or category) detector for the reddit posts, the data should have uniform representation of the fields (or flares) into which the posts are categorised in Reddit.\n",
    "    - Created a Reddit account (sorry)\n",
    "    - Exploring Reddit and what are the different types of flares (and how are they labelled)\n",
    "    - After some analysis, realised some posts categories might not as easy to predict (for a human either) so accuracy can not be expected to be 90-95%+\n",
    "    - observed which features helped me predict the flair of a post. got confused about certain examples:\n",
    "        - \"Open letter from Kamal Haasan to Prime Minister Narendra Modi\" \n",
    "        \n",
    "            marked coronavirus. does one political member writing a letter to another member come under politics? the post contained a link to an article about Kamal writing about corona to PM, so did flaring of this post involved reading the article (i.e. URL) linked in the post?\n",
    "        - \"Muslim Truckers 'Beaten Up' in Arunachal, Concern Over Supplies of Essential Items\" \n",
    "        \n",
    "            marked: politics. why is this post marked Politics even though it majorly deals with the communal problem caused by Tablighi Jamaat's Markaz related coronavirus cases. (article link worth citing for referencing while developing model: https://www.news18.com/news/india/muslim-truckers-beaten-up-in-arunachal-concern-over-supplies-of-essential-items-2565619.html) \n",
    "            \n",
    "    - Conclusion of cursory understanding of the posts and flares: Most posts currently related to coronavirus and politics -> possible skewness in the data that will be collected. Photography, Science/Technology (suprisingly) have relatively lowest posts.\n",
    "- read about reddit flairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 2 : April 8, 2020 (T - 18 days)\n",
    "\n",
    "To write a script to extract post headline, content and category/flair\n",
    "\n",
    "- created a virtual environment, since I messed up my last laptop with version conflicts of python and other softwares, I decided to learn to use virtual environments for this project! Happy programming to me! (The easiest setup guide for venv - https://gist.github.com/pandafulmanda/730a9355e088a9970b18275cb9eadef3)\n",
    "- https://towardsdatascience.com/scraping-reddit-data-1c0af3040768 - is a good link for creating a web scrapping API for Reddit using PRAW. (used this link to develop the first data extraction notebook - later faced a couple of errors since post flairs were not evenly distributed)\n",
    "#### First problem: Flair extraction\n",
    "- Having spent 2 hours trying to get around extracting the flairs of the posts, it seems weirdly impossible. I have a feeling the solution is pretty basic but this is getting frustrating\n",
    "- After getting too excited about the visibility of a flair, realised I found a way to extract possible flairs and not individual post flairs. \n",
    "#### Lesson 1: Don't get too excited when you find something you've been scavenging for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 3 : April 9, 2020 (T - 17 days)\n",
    "- found out about the term 'submission' and then found how to extract the flair of a submission using post urls that were easily attainable.\n",
    "- problem: some reddit posts which contained just the headline and a link, for those posts the url extracted is not the url of the reddit post, but of the link contained in the post\n",
    "- now, need a way to extract the url of the reddit post irrespective of the content it contains.\n",
    "- reading the documentation of reddit.submission I realised submissions can be extracted using post ids too. urls failed me but post id has to be of the reddit post only. \n",
    "#### Problem 1 solved! \n",
    "- So i extracted submissions now using their id and used the previous submission.link_flair_text and got the flair of the submission :D\n",
    "- combined with the existing url, id, headline and content to form the resultant data (for now) :D\n",
    "- Finally Task 1 done, I created a data extraction.ipynb notebook and commited it in the github repository.\n",
    "Onto the second task now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 4 & 5 : April 10-11, 2020 (T - 16 days)\n",
    "Start the second task: Exploratory data analysis\n",
    "- started understanding the data distribution\n",
    "- removed the url and id for now (since they don't add anything concrete to the training process)\n",
    "- found the rows containing content values as not null\n",
    "- to plot the data, started reading some analysis notebooks for good visualisations (https://www.kaggle.com/artgor - what a Kaggler!)\n",
    "- Planned to analyse the flair distributions, plot them and see if data procurring requires any changes (my guess is it will since I saw a lot of posts marked coronavirus, and less of photography or science/technology)\n",
    "- guess what right! haha!\n",
    "- Also, there is a party emoji in one flair. After some unicode struggles, found and removed it! Still pretty weird!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 6 : April 12, 2020 (T - 14 days)\n",
    "\n",
    "- decided to print the possible official flairs in indian subreddit\n",
    "- plotted the pie chart to see how many posts had no content, to see how valuable 'content' attribute was (about 70% posts had no content, oho)\n",
    "- plotted bar graph to see the distribution of the official flairs in the data (only 3 out of the 12 categories had some workable frequency of flairs - disappointing)\n",
    "- decided to analyse the 'title' attribute too. would find the mean length of the title and plot a histogram to see its variation\n",
    "- started reading different kaggle kernels of grandmasters to see how they visualise and explore data. Spent some good amount of time on https://www.kaggle.com/artgor/eda-and-models kernel, still haven't finishing the data exploring section, good read\n",
    "#### Probelm 2: Data collection needs revision!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 7 - 10 : April 13 - 16, 2020 (T - 13 days)\n",
    "\n",
    "Last night while searching for ways to extract posts according to post flairs from subreddit, I accidently found some reddit flair detectors. These were some github repositories that did exactly what the task was. Decided to not hover over them for a couple of days at least. \n",
    "\n",
    "- read the READMEs of these github repositories to get an idea of how to write a README of the repo. taking note of file structures, instructions on running the file.\n",
    "#### Solution found!\n",
    "- found a better way to scrap submissions: based on flairs using submission.search!\n",
    "- fixed the data exploration by finding a method to search submissions based on post flairs.\n",
    "- created a preferred data extraction file and re-ran data exploration file for the new dataset\n",
    "- committed data exploration file\n",
    "- starting with data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 13 - 14 : April 19 - 20 , 2020 (T - 7 days)\n",
    "- applied basic text pre-processing (like stop word removal etc) on all attributes with text fields (title and content)\n",
    "- read my previous notebooks on how I converted textual features to vectors for training (found and read the implementation for bag of words and word2vec approach)\n",
    "- with the literature survey, found two things:\n",
    "    #### Interesting learning\n",
    "    - one particular project had used the bag of words method with basic ML algorithms very efficiently (want to replicate that)\n",
    "#### Minor problem 3:\n",
    "    - most projects used more attributes than just title and content, they used urls (of the post or in the post, not sure?), top comments and analysed model performances when using each of these separately to train and test and when combined a certain set of these attributes and compared the performances of different algorithms on these features.\n",
    "- next steps include:\n",
    "    - extracting URLs and top comments\n",
    "    - pre-process and vectorise the textual features\n",
    "    - train and build classical ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 15 : April 21, 2020 (T - 5 days)\n",
    "- decided to colllect url and comments later\n",
    "- focusing on training the model on at least the pre-existing features\n",
    "#### Learning something new!\n",
    "- implementing and discoverign the beautiful feature of Pipeline in sklearn! (https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "    - Pipeline taken parameters of a list of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an estimator.\n",
    "    - compared the pipeline feature with my previous NLP project to observe its validity. and it was infact true. I first used count vectoriser, then tfidf and finally a estimator (ML algorithm)\n",
    "    #### Thoughts for future!\n",
    "    - I'm thinking of implementing and comparing the performing with an unsupervised algorithm (like K-Means too) once I've tested these. \n",
    "- started trying different combination of features to train and validate the models, storing the results in a spreadsheet\n",
    "- So far I tried three attributes, title, content and url\n",
    "- title + loghistic regression gave 68% accuracy, and title + content + url gave 69% accuracy on Random Forests (althought it was a little wird since other algorithms on this feature combination gave not-so-good results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 16 - 17 : April 22 - 23, 2020 (T - 4 days)\n",
    "- decided to extract the comments today and try combining comments with the models\n",
    "#### frustrating days + nights\n",
    "- what a pain to download so many posts again. Already spent a night + morning on this. \n",
    "- Finally have the comments. But weirdly it contains a lot of NaNs. Since model cannot use these NaNs, I have to drop these rows.\n",
    "#### Problem 4:\n",
    "- dropping NaN rows leaves me with hardly half the dataset, which is such a bummer. the results are dropping, I don't like this.\n",
    "\n",
    "- Compared with how the sample github repo coded things up and to see how they dealt with nan. its so weird, they didn't have any nan in their dataset, idk how it is possible. I would have scanned more about it, but short on time.\n",
    "- https://www.youtube.com/watch?v=UbCWoMf80PY is a good link to be introduced on how to deploy your ml models using flask (got some basic idea from this video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 18 - 20 : April 24 - 26 \n",
    "\n",
    "#### Beautiful Learnings!\n",
    "- Learnt to build my website (since this is a very simple project, it was an easy task)!\n",
    "- Used Flask for API - finally coding in flask independently, it was a little struggle but great fun!\n",
    "#### Problem 5:\n",
    "- faced some serious error with understanding how the object is being passed through request.form\n",
    "#### Solution\n",
    "- realised the pipeline works fine! It is some error in handling the object passed\n",
    "- decided to display the object contents\n",
    "- realised it is a multidict \n",
    "- unrolled it and displayed its contents to find where the url was stored!!\n",
    "\n",
    "#### Weird discoveries:\n",
    "While working on understanding the predictions on local machine, I discovered a lot of weird things. \n",
    "1. so the title mostly has the class in it in one way or the other, which is why the model performed surprisingly well on the titles.\n",
    "2. Some posts had incorrect flairs when they were extracted in comparison to what the reddit handle had labelled. For example post number 701 in the dataset, Title: different stages hair loss perfect order mumbai local train memories, Reddit url: https://www.reddit.com/r/india/comments/g4200g/different_stages_of_hair_loss_in_perfect_order/ , extracted Flair: Sports, Actual Flair (on Reddit): Photography. This is extremely weird and idk how many such cases are there.\n",
    "\n",
    "\n",
    "#### Problem 6:\n",
    "- while deploying, I thought it'd be easy but:\n",
    "    1. While deploying my application at heroku, certain requirements showed the error 'ERROR: Could not find a version that satisfies the requirement XX'. I tried googling for individual lines and other than this (https://stackoverflow.com/questions/47304291/heroku-upload-could-not-find-a-version-that-satisfies-the-requirement-anaconda) I could not find any concrete answer. The answer essentially asks me to delete the requirements for which this error pops up, push the code again and re-deploy, everytime. Since these errors popped up for some non-essential libraries (labelling them non-essential from a visible interaction point of view), I could do as mentioned above and move on. But the doubt still remains: what is the conflict? Why does freeze put some requirements in the txt file that heroku could not install?  \n",
    "    2. Problem: So after the requirements were downloaded from requirements.txt, Heroku started downloading some packages by itself. These packages were mentioned in requirements.txt. On one such installation, it showed the error attached in the image below. I've been trying to Google around to find a solution to this problem since yesterday but haven't been successful, so far. \n",
    "\n",
    "#### Solution found:\n",
    "- After googling a lot of different errors in different ways, I decided to chop off any requirements that were not being directly used. committed, and re-deployed everything!! MAGICAL! it worked! (Still haven't found the answer to either of those two questions though!)\n",
    "- deployed the web app and yay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsolved Problem:\n",
    "- after trying for 3-4 hours, with the deadline piling up I could not automate the testing for this application. I have an unfinished code. It has a small error (so far) but unit and integration testing would need time. So would probably try it tomorrow!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
